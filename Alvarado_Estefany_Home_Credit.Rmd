---
title: "Capstone Project - Home Credit Default Risk"
author: "Estefany Alvarado"
date: "2024-12-06"
output: html_document
---

# Task 1: Data exploration

```{r}
# Load necessary libraries
library(ggplot2)  
library(dplyr) 
library(tidyverse)
library(readr)    
library(tidyr) 
library(stats)
library(psych)
library(corrplot)
library(reshape2)
library(purrr)  
library(fastDummies)
library(skimr)
library(janitor)
library(caret)
library(pROC)
library(randomForest)
library(xgboost)
#install.packages("glmnet")
library(glmnet)
#install.packages("DescTools")
library(DescTools)
# install.packages("themis")
# install.packages("janitor")
library(themis)
library(recipes)
library(janitor)
#options(repos = c(CRAN = "https://cran.rstudio.com"))
#install.packages("gbm")
library(gbm)
# Load necessary libraries
library(e1071)      # For Naive Bayes
library(caret)      # For F1-Score and confusion matrix
#install.packages("PRROC")
library(PRROC)      # For Precision-Recall Curve
#install.packages("lightgbm")
library(lightgbm)
#install.packages("ROSE")
library(ROSE)
#install.packages("DMwR2")
library(DMwR2)
#install.packages("smotefamily")
library(smotefamily)
library(ROSE)
library(e1071)
# Suppress warnings
options(warn = -1)
```
This chunk loads libraries essential for data analysis, cleaning, visualization, machine learning, and statistical modeling.

```{r}
# Load the datasets
application_train <-read_csv("application_train.csv", show_col_types = FALSE)
application_test <- read_csv("application_test.csv", show_col_types = FALSE)
```

Load two datasets: application_train for training models and application_test for making predictions.

```{r}
# Convert column names to lowercase
colnames(application_train) <- tolower(colnames(application_train))
colnames(application_test) <- tolower(colnames(application_test))
```

Standardizes column names by converting them to lowercase for consistency and to avoid issues with case sensitivity in later operations.

This initial exploration will guide the preprocessing steps.


## Train dataset exploration

```{r}
# Dataset Dimensions
# Number of rows and columns for train dataset
num_rows <- nrow(application_train)
num_cols <- ncol(application_train)

cat("Number of rows:", num_rows, "\n")
cat("Number of columns:", num_cols, "\n")
```
Train dataset contains 307511 observations and 122 predictors, including the target variable which is numeric.

```{r}
# Identifying Missing Values
# Display only columns with missing values and their counts
missing_values <- colSums(is.na(application_train))
missing_values <- missing_values[missing_values > 0]

# Print the result
missing_values

# Count the number of variables with missing values
missing_count <- sum(colSums(is.na(application_train)) > 0)

# Print the result
cat("Number of variables with missing values:", missing_count, "\n")

```
67 columns have missing values

## Test dataset exploration 

```{r}
# Number of rows and columns in test dataset
num_rows_t <- nrow(application_test)
num_cols_t <- ncol(application_test)

cat("Number of rows:", num_rows_t, "\n")
cat("Number of columns:", num_cols_t, "\n")
```
Test dataset contains 48744 observations and 121 predictors. This dataset doesn't have the target variable.

```{r}
# Display only columns with missing values and their counts
missing_values <- colSums(is.na(application_test))
missing_values <- missing_values[missing_values > 0]

# Print the result
missing_values

# Count the number of variables with missing values
missing_count <- sum(colSums(is.na(application_test)) > 0)

# Print the result
cat("Number of variables with missing values:", missing_count, "\n")

```
64 columns have missing values.


# Task 2: Analyzing the target variable and predictor relationships

```{r}
#Check the balance of the target variable.
table(application_train$target)

# Proportion of target variable
prop.table(table(application_train$target))

```
The target variable is highly imbalanced, with 282686 observations labeled as '0' (No Default) and only 24825 labeled as '1' (Default). 

```{r}
# Summarize the class distribution
class_distribution <- table(application_train$target)

# Convert to a data frame for plotting
class_df <- as.data.frame(class_distribution)
colnames(class_df) <- c("Class", "Count")

# Create the bar plot
ggplot(class_df, aes(x = factor(Class), y = Count, fill = factor(Class))) +
  geom_bar(stat = "identity") +
  labs(
    title = "Class Imbalance Plot",
    x = "Class",
    y = "Count",
    fill = "Class"
  ) +
  theme_minimal()
```

We can notice a class imbalance: the number of clients who do not default (92%) is greater than the number of clients who default(8%).


```{r}
#Calculate the accuracy of a majority class predictor.
majority_class_accuracy <- max(table(application_train$target)) / nrow(application_train)
majority_class_accuracy
```

The accuracy of a naive majority class predictor is calculated as 91.93%, which is expected given the class imbalance.

To address this,  will explore balancing techniques such as SMOTE or undersampling in the modeling phase.

# Task 3: Data preparation  

## Handle missing values for numeric and categorical variables

```{r}
# Summarize missing data only for columns with missing values and round to 2 decimals
missing_data <- application_train %>%
  clean_names() %>%  # Ensures column names are cleaned (if not already)
  summarise(across(everything(), ~sum(is.na(.)) / n())) %>%
  pivot_longer(cols = everything(), names_to = "column", values_to = "missing_fraction") %>%
  filter(missing_fraction > 0) %>%  # Keep only columns with missing values
  mutate(missing_fraction = round(missing_fraction, 4))  # Round to 2 decimal places

# View the result
print(missing_data, n = Inf)  # Show all rows in the console
```

```{r}
# Identify the most problematic columns
missing_data <- missing_data %>% arrange(desc(missing_fraction))
print(missing_data, n = Inf)

```

```{r fig.height=10, fig.width=10}
#Visualize missing values
ggplot(missing_data, aes(x = reorder(column, -missing_fraction), y = missing_fraction)) +
  geom_bar(stat = "identity", aes(fill = missing_fraction), color = "black") +
  coord_flip() +  # Flip coordinates for readability
  scale_fill_gradient(low = "skyblue", high = "steelblue", name = "Missing Fraction") +
  labs(
    title = "Proportion of Missing Values by Column",
    x = "Column",
    y = "Missing Fraction"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 8),  # Smaller text for y-axis labels
    axis.title.y = element_text(size = 10),
    axis.title.x = element_text(size = 10),
    plot.title = element_text(hjust = 0.5, size = 14)  # Center the title
  )

```

```{r}
# Dropp columns with missing values that are weakly correlated with the target variable 

# Step 1: Identify columns with missing values
missing_cols <- names(application_train)[colSums(is.na(application_train)) > 0]

# Step 2: Calculate correlations for numeric columns
correlations <- application_train %>%
  select(all_of(missing_cols), target) %>%  # Select columns with missing values and target
  select(where(is.numeric)) %>%  # Keep only numeric columns for correlation
  cor(use = "pairwise.complete.obs")  # Correlation matrix (pairwise for handling missing values)

# Extract correlations with the target variable
target_correlations <- correlations["target", ] %>% na.omit()

# Step 3: Filter columns with low correlation (e.g., abs(correlation) < 0.05)
low_corr_cols <- names(target_correlations[abs(target_correlations) < 0.05])

# Step 4: Drop columns with low correlation and missing values
columns_to_drop <- intersect(missing_cols, low_corr_cols)
application_train <- application_train %>% select(-all_of(columns_to_drop))

# View the columns dropped
cat("Dropped columns:", paste(columns_to_drop, collapse = ", "), "\n")


```
So, approximately 80.6% (54/67) of the columns with missing values were removed. The remaining 13 columns with missing values were likely retained due to their correlation with the target variable.

```{r}
# Identify the remaining columns with missing values
remaining_missing_cols <- setdiff(missing_cols, columns_to_drop)
# Check their missing value counts
remaining_missing_values <- colSums(is.na(application_train[remaining_missing_cols]))
print(remaining_missing_values)

```
```{r}
# Calculate numeric columns correlation with the target variable

# Filter remaining numeric columns with missing values
numeric_remaining_cols <- remaining_missing_cols[remaining_missing_cols %in% names(application_train %>% select(where(is.numeric)))]

# Extract correlations for remaining numeric columns with the target variable
numeric_remaining_correlations <- target_correlations[numeric_remaining_cols]

# Print the correlations
cat("Correlations with the target variable (numeric columns only):\n")
print(numeric_remaining_correlations)

```
* ext_source_1, ext_source_2, and ext_source_3 have notable correlations with the target variable (negative correlations of around -0.15 to -0.18).These numeric columns should be retained, and their missing values can be imputed (e.g., using the median or a predictive model).

* days_last_phone_change has a weak positive correlation (0.055), which could still be retained depending on its importance to the model.

1. **ext_source_1, ext_source_2, ext_source_3:** Since these features are continuous and highly correlated with the target variable, their missing values can be imputed using the median, which is robust to outliers.

```{r}
# Recommendation

application_train$ext_source_1 <- ifelse(is.na(application_train$ext_source_1), 
                                         median(application_train$ext_source_1, na.rm = TRUE), 
                                         application_train$ext_source_1)

application_train$ext_source_2 <- ifelse(is.na(application_train$ext_source_2), 
                                         median(application_train$ext_source_2, na.rm = TRUE), 
                                         application_train$ext_source_2)

application_train$ext_source_3 <- ifelse(is.na(application_train$ext_source_3), 
                                         median(application_train$ext_source_3, na.rm = TRUE), 
                                         application_train$ext_source_3)

```

2. **days_last_phone_change:** Since this variable might represent a time duration, considered imputing missing values with the median to preserve the central tendency of the data.

```{r}
application_train$days_last_phone_change <- ifelse(is.na(application_train$days_last_phone_change), 
                                                   median(application_train$days_last_phone_change, na.rm = TRUE), 
                                                   application_train$days_last_phone_change)

```


```{r}
# # Calculate categorical columns association with the target variable

# Filter remaining categorical columns with missing values
categorical_remaining_cols <- remaining_missing_cols[!(remaining_missing_cols %in% names(application_train %>% select(where(is.numeric))))]

# Print the categorical columns
cat("Remaining categorical columns with missing values:\n")
print(categorical_remaining_cols)
```
```{r}

# Perform Chi-Square test for each categorical column
chi_square_results <- lapply(categorical_remaining_cols, function(col) {
  chisq.test(table(application_train[[col]], application_train$target), simulate.p.value = TRUE)
})

# Extract p-values from the test
chi_square_pvalues <- sapply(chi_square_results, function(x) x$p.value)

# Print p-values
names(chi_square_pvalues) <- categorical_remaining_cols
cat("Chi-Square Test p-values for remaining categorical columns:\n")
print(chi_square_pvalues)

```
A p-value < 0.05 indicates a statistically significant association between the variable and the target.
These categorical variables are likely important predictors for the model and should be retained for further analysis.


```{r}
categorical_cols <- c("name_type_suite", "occupation_type", "fondkapremont_mode",
                      "housetype_mode", "wallsmaterial_mode", "emergencystate_mode")

for (col in categorical_cols) {
  p <- ggplot(application_train, aes_string(x = col)) +
    geom_bar(fill = "steelblue", color = "black") +
    labs(title = paste("Distribution of", col), x = col, y = "Count") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)  # Print the plot within the loop
}

```
1. **Name_type_suite:** There are a moderate number of missing values (NA), which might indicate the absence of a specific "suite" type during application. Here I will impute with "Unaccompanied", since it's the most common category, it might be a reasonable assumption.

```{r}
# Recommendation
application_train$name_type_suite <- replace_na(application_train$name_type_suite, "Unaccompanied") 

```

```{r}
# Replace NA values in the variable with "Unaccompanied"
application_test$name_type_suite[is.na(application_test$name_type_suite)] <- "Unaccompanied" # Im doing this now to don't have issues when one hot encode the dataset.
```
2. **occupation_type:** There is a significant proportion of NA values. These could indicate:

* Individuals without recorded employment (e.g., students, unemployed).

* Missing data during collection.

Here, as a solution I will impute Missing Values with "Unemployed/Missing", creating a new category to retain the missingness information.

```{r}
# Recommendation
application_train$occupation_type <- replace_na(application_train$occupation_type, "Unemployed/Missing")
```

3. **fondkapremont_mode:** This variable likely reflects how maintenance funds are managed, and NA might indicate that the applicant does not contribute to such a fund. I will impute missing values with "Not Applicable", since NA might indicate no participation in maintenance funds, this would be logical.

```{r}
# Recommendation
application_train$fondkapremont_mode <- replace_na(application_train$fondkapremont_mode, "Not Applicable")
```

4. **housetype_mode:** "Block of flats" is the dominant category, with most other categories being underrepresented. A large proportion of NA values might mean that housing type data was not collected for some applicants.For this case, I will mpute with "Missing/Not Specified", retaining the missing information as a category.

```{r}
# Recommendation
application_train$housetype_mode <- replace_na(application_train$housetype_mode, "Not Specified")
```

5. **wallsmaterial_mode:** "Panel" and "Stone, brick" dominate the distribution, with a significant number of NA values. NA might mean no information about wall material was collected. I will impute Missing Values with "Not Specified", so this retains the missingness information.

```{r}
# Recommendation
application_train$wallsmaterial_mode <- replace_na(application_train$wallsmaterial_mode, "Not Specified")

```

6. **emergencystate_mode:** "No" dominates the distribution, and very few entries are "Yes." NA might indicate that the emergency state status was not recorded or is not applicable.I will impute Missing Values with "No", since "No" is overwhelmingly common, it’s a practical choice.

```{r}
# Recommendation
application_train$emergencystate_mode <- replace_na(application_train$emergencystate_mode, "No")
```


## Summary of actions taken

**1. Numeric columns:** Imputed ext_source_1, ext_source_2, ext_source_3, and days_last_phone_change with their respective median values due to their correlations with the target variable.

**2. Categorical columns:** Performed Chi-Square tests and found all categorical variables to have statistically significant associations with the target variable (p-value < 0.05).

* **Imputed:**

* name_type_suite with "Unaccompanied" (most common value).

* occupation_type with "Unemployed/Missing" (to capture missingness information).

* fondkapremont_mode with "Not Applicable" (logical assumption based on missingness).

* housetype_mode with "Not Specified" (to retain missingness information).

* wallsmaterial_mode with "Not Specified" (similar reasoning as above).

* emergencystate_mode with "No" (dominant value).\


# Task 4: Feature importance and selection

Given the size of the train dataset (307,511 observations and 65 variables) and the current workflow, I decided to perform feature importance selection first and then balance the dataset for the following reasons:

* The dataset is large, and balancing it before feature selection could significantly increase its size (e.g., oversampling with SMOTE).

* Reducing the number of features first will make subsequent steps (like balancing and model training) faster and more efficient.

* If synthetic data influences feature importance, it could lead to overemphasis on certain features that might not generalize well.

* Balancing the dataset (especially with SMOTE or oversampling) can introduce synthetic data. Performing feature selection afterward ensures only include features that genuinely contribute to the model’s predictions.

```{r fig.height=10, fig.width=10}
# Ensure the target Variable is a Factor
application_train$target <- as.factor(application_train$target)

# Ensure levels are properly defined as 0 and 1
application_train$target <- factor(application_train$target, levels = c("0", "1"))

# Random Forest Feature Importance
set.seed(123)
rf_model_p <- randomForest(target ~ ., data = application_train, importance = TRUE, ntree = 50)
importance <- importance(rf_model_p)
varImpPlot(rf_model_p)

# Select top 25 features
important_vars <- names(sort(importance[, "MeanDecreaseGini"], decreasing = TRUE)[1:25])
reduced_data <- application_train[, c(important_vars, "target")]

```


```{r}
print(important_vars)
```

* sk_id_curr can be excluded as it serves no predictive purpose.

Feature importance was assessed using a Random Forest model. Variables like "ext_source_2" and "days_birth"  showed high importance, indicating their strong predictive power for default predictions.

```{r}
# Drop the 'sk_id_curr' column from the dataset
reduced_data <- reduced_data[, !names(reduced_data) %in% "sk_id_curr"]

# Check if the column has been dropped
print("sk_id_curr" %in% names(reduced_data))  # Should return FALSE

```

```{r}
# View the column names
names(reduced_data)

```

# Task 5: Handle outliers

Focus in numeric columns:

```{r}
# Identify numeric columns
numeric_cols <- names(reduced_data)[sapply(reduced_data, is.numeric)]

```

Identify outliers for each numeric column:

```{r}

# Using boxplots
# Loop through numeric columns to create boxplots
for (col in numeric_cols) {
  print(
    ggplot(reduced_data, aes_string(y = col)) +
      geom_boxplot(fill = "steelblue") +
      labs(title = paste("Boxplot of", col), y = col) +
      theme_minimal()
  )
}

```

```{r}

# Using IQR Method
# Function to detect outliers based on IQR
detect_outliers <- function(data, col) {
  q1 <- quantile(data[[col]], 0.25, na.rm = TRUE)
  q3 <- quantile(data[[col]], 0.75, na.rm = TRUE)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  data[data[[col]] < lower_bound | data[[col]] > upper_bound, ]
}

# Detect outliers for each numeric column
outliers <- lapply(numeric_cols, function(col) detect_outliers(reduced_data, col))

# Count the number of outliers in each column
outlier_counts <- sapply(outliers, nrow)
print(outlier_counts)

```
The numeric variables with outliers are:

* **ext_source_3 (4313):** Outliers are present near the lower bound (close to 0).
```{r}
# Recommendation
# Cap outliers at the nearest boundary (0). Winsorize the upper bound as well
reduced_data$ext_source_3 <- pmax(reduced_data$ext_source_3, 0)
reduced_data$ext_source_3 <- Winsorize(reduced_data$ext_source_3)
```

* **days_registration(659):**  Extreme outliers on the lower end (very negative values).Number of days since registration, with negatives indicating past dates.

```{r}
# Recommendation
# Cap extreme values using the 1.5 × IQR boundary
reduced_data$days_registration <- Winsorize(reduced_data$days_registration)
```

* **amt_credit(6562):** Upper outliers, possibly very large credit amounts.

```{r}
# Recommendation
# Apply log1p() to reduce skewness and compress large values
reduced_data$amt_credit <- Winsorize(reduced_data$amt_credit)
```


* **days_last_phone_change (435):** Lower bound outliers, with very negative values. Days since last phone change (negative means past).

```{r}
# Recommendation
# Replace outliers with the median, assuming very old phone changes are less meaningful.

median_value <- median(reduced_data$days_last_phone_change, na.rm = TRUE)
reduced_data$days_last_phone_change <- pmax(reduced_data$days_last_phone_change, median_value)

```


* **days_employed(72217):** this variables has 72217 observations with one positive outlier 365243.

```{r}
# Recommendation
# Replace the extreme value with NA
reduced_data$days_employed[reduced_data$days_employed == 365243] <- NA
# Replace NA values with the median
reduced_data$days_employed <- ifelse(is.na(reduced_data$days_employed),
                                     median(reduced_data$days_employed, na.rm = TRUE),
                                     reduced_data$days_employed)

# After replace with median still having 23148 observations with outliers.
# Calculate IQR, Q1, and Q3
q1 <- quantile(reduced_data$days_employed, 0.25, na.rm = TRUE)
q3 <- quantile(reduced_data$days_employed, 0.75, na.rm = TRUE)
iqr <- q3 - q1

# Define bounds
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr

# Winsorize outliers
reduced_data$days_employed <- ifelse(reduced_data$days_employed < lower_bound, lower_bound, 
                                     ifelse(reduced_data$days_employed > upper_bound, upper_bound, 
                                            reduced_data$days_employed))


```

* **amt_income_total(14035):** Extreme upper outliers

```{r}
# Recommendation
# log tranformation: compress the skewed distribution to better handle extremes
reduced_data$amt_income_total <- log1p(reduced_data$amt_income_total)
reduced_data$amt_income_total <- Winsorize(reduced_data$amt_income_total)
```

* **region_population_relative(8412):** A upper outlier (0.06)

```{r}
# Recommendation
# Cap the outliers using the 1.5 × IQR boundary, as it’s likely valid but extreme
reduced_data$region_population_relative <- Winsorize(reduced_data$region_population_relative)
```

* **ext_source_1(134134):**  Extreme values near both bounds.

```{r}
# Recommendation

# Calculate IQR, Q1, and Q3
q1 <- quantile(reduced_data$ext_source_1, 0.25, na.rm = TRUE)
q3 <- quantile(reduced_data$ext_source_1, 0.75, na.rm = TRUE)
iqr <- q3 - q1

# Define bounds
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr

# Winsorize outliers
reduced_data$ext_source_1 <- ifelse(reduced_data$ext_source_1 < lower_bound, lower_bound, 
                                    ifelse(reduced_data$ext_source_1 > upper_bound, upper_bound, 
                                           reduced_data$ext_source_1))




```

* **hour_appr_process_start(2257):** Small number of outliers near the lower and upper bounds.

```{r}
# Recommendation
#  Capping outliers at the 1.5 × IQR boundary
reduced_data$hour_appr_process_start <- Winsorize(reduced_data$hour_appr_process_start)
```

* **cnt_children(4272):**Few extreme outliers with large numbers of children. While it's possible to have more children, such cases are rare and could introduce noise into the model.

```{r}
# Recommendation
#  Cap values above a realistic threshold (e.g. 10 children).
#reduced_data$cnt_children <- pmin(reduced_data$cnt_children, 10) this didnt work.
# Remove rows with extreme values #reduced_data <- reduced_data[reduced_data$cnt_children <= 10, ] this didnt work either.

q1 <- quantile(reduced_data$cnt_children, 0.25, na.rm = TRUE)
q3 <- quantile(reduced_data$cnt_children, 0.75, na.rm = TRUE)
iqr <- q3 - q1
upper_bound <- q3 + 1.5 * iqr
reduced_data$cnt_children[reduced_data$cnt_children > upper_bound] <- upper_bound

```


```{r}
# Identify numeric columns
numeric_cols <- names(reduced_data)[sapply(reduced_data, is.numeric)]

# Recheck for remaining outliers
outlier_counts <- sapply(numeric_cols, function(col) {
  Q1 <- quantile(reduced_data[[col]], 0.25, na.rm = TRUE)
  Q3 <- quantile(reduced_data[[col]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  sum(reduced_data[[col]] < lower_bound | reduced_data[[col]] > upper_bound, na.rm = TRUE)
})
print(outlier_counts)
```
# Task 6: Handle class imbalance

## Conversion of categorical columns to factors

```{r}
# Convert categorical columns in reduced_data to factors
reduced_data <- reduced_data %>%
  mutate(across(where(is.character), as.factor))

```

```{r}
# Lets do the same for the application_test dataset
application_test <- application_test %>%
  mutate(across(where(is.character), as.factor))
```


```{r}
# Separate the target variable fromt the reduced_data dataset
target <- reduced_data$target
# reduced_data without the target variable
predictors <- reduced_data[, -which(names(reduced_data) == "target")]

```


```{r}
# Check the predictors from the reduced_data dataset
str(predictors)
```

```{r}
# Check the predictors from the application_test dataset
str(application_test)
```

```{r}
# Check for factor columns in the predictors dataset
factor_columns <- names(Filter(is.factor, predictors))
print(factor_columns)

```
The goal in this part of the code is to ensure that the predictors in the application_test dataset match those in the reduced data, as this alignment will be necessary later for testing the best model.

```{r}
# Ensure the same column names
common_columns_test <- intersect(colnames(application_test), colnames(predictors)) # since application_test doesn't have the target variable

# Filter application_test to keep only common columns
application_test_reduced <- application_test[, common_columns_test, drop = FALSE]

# Reorder columns in application_test to match predictors
application_test_reduced <- application_test_reduced[, colnames(predictors), drop = FALSE]

# Check if the columns match
all_columns_match <- all(colnames(application_test_reduced) == colnames(predictors))
cat("Do all columns match? ", all_columns_match, "\n")

```

```{r}
# Check for factor columns in the application_test dataset
factor_columns_application_test <- names(Filter(is.factor, application_test))
print(factor_columns_application_test)

```
## Apply one hot encoding

```{r}
# One-hot encode categorical variables in the predictors data
encoded_predictors <- dummy_cols(
  predictors, 
  select_columns = factor_columns, 
  remove_selected_columns = TRUE, 
  remove_first_dummy = TRUE  # Avoid multicollinearity
)

# Check the structure of the encoded dataset
str(encoded_predictors)

```

```{r}
# One-hot encode categorical variables for the application_test
application_test_encoded <- dummy_cols(
  application_test_reduced, 
  select_columns = factor_columns_application_test, 
  remove_selected_columns = TRUE, 
  remove_first_dummy = TRUE  # Avoid multicollinearity
)

# Check the structure of the application test encoded dataset
str(application_test_encoded)

```

```{r}
# identify columns that don't match between predictors and application_test
setdiff(colnames(encoded_predictors), colnames(application_test_encoded))

```

```{r}
# identify columns that don't match between application_test and predictors
setdiff(colnames(application_test_encoded), colnames(encoded_predictors))

```
When handling missing values, I decided to include "Missing" or "Not Specified" in the columns that lacked values. Now, I will rename the columns in the application_test dataset as follows: "occupation_type_NA" to "occupation_type_Unemployed/Missing," "wallsmaterial_mode_NA" to "wallsmaterial_mode_Not Specified," "name_type_suite_NA" to "name_type_suite_Unaccompanied," and "fondkapremont_mode_NA" to "fondkapremont_mode_not specified" to ensure alignment with the encoded_predictors dataset.


```{r}
# Rename columns in application_test_encoded
application_test_encoded <- application_test_encoded %>%
  rename(
    "occupation_type_Unemployed/Missing" = "occupation_type_NA",
    "wallsmaterial_mode_Not Specified" = "wallsmaterial_mode_NA",
    "fondkapremont_mode_not specified" = "fondkapremont_mode_NA",
  )

```


```{r}
# identify again columns that don't match between predictors and application_test
setdiff(colnames(encoded_predictors), colnames(application_test_encoded))

```
These two columns are not present in application_test_encoded because there are no levels with these names in the application_test_encoded dataset.


```{r}
# Add the target variable back to the dataset (encoded_predictors)
final_train <- cbind(encoded_predictors, target)

# Verify the structure of the final dataset
str(final_train)

```


```{r}
# Identify and remove near-zero variance predictors
nzv <- nearZeroVar(final_train, saveMetrics = TRUE)
final_train <- final_train[, !nzv$nzv]
```


```{r}
str(final_train)
```

* The dataset initially had 134 variables. After removing near-zero variance predictors, only 47 variables remain. 87 variables had little to no variability and were removed as they were unlikely to contribute to the predictive power of the model.

* Reducing the number of predictors decreases computational complexity and speeds up model training.

* Features with little variability provide little information to the model and can lead to overfitting, especially in flexible algorithms like Random Forest or XGBoost.



The goal in this part of the code is to ensure that the predictors in the application_test dataset match those in the final data, as this alignment will be necessary later for testing the best model.

```{r}
# Remove the 'target' column from final_train
final_train_no_target <- final_train[, !colnames(final_train) %in% "target"]

# Ensure columns in application_test_encoded match those in final_train_no_target
common_columns_test_encoded <- intersect(colnames(final_train_no_target), colnames(application_test_encoded))

# Subset application_test_encoded to keep only the common columns
application_test_encoded <- application_test_encoded[, common_columns_test_encoded, drop = FALSE]

# Ensure column order matches final_train_no_target
application_test_encoded <- application_test_encoded[, colnames(final_train_no_target)]

# Verify alignment
identical(colnames(application_test_encoded), colnames(final_train_no_target))

```

```{r}
str(application_test_encoded)
```


```{r}
# Clean column names
colnames(final_train) <- make.names(colnames(final_train))
colnames(application_test_encoded) <- make.names(colnames(application_test_encoded))
```

At this point final_train has 47 variables (including the target variable) and application_test_encoded has 46 variables(no target).

# Step 7: Split the application_train(final_train) dataset in to train and test sets

```{r}
# Set target and predictors
target <- final_train$target
predictors <- final_train %>% select(-target)

```

```{r}
# Create a stratified split
set.seed(123)
train_index <- createDataPartition(target, p = 0.8, list = FALSE)
train_data <- final_train[train_index, ]
test_data <- final_train[-train_index, ]

# Separate predictors and target in training and testing sets
train_x <- train_data %>% select(-target)
train_y <- train_data$target
test_x <- test_data %>% select(-target)
test_y <- test_data$target

```


# Step 8: Model implementation

**Plan for model implementation**

**1. Naive Bayes:**

* **Option 1:** Adjust class priors manually to handle imbalance.

* **Option 2, 3 and 4:** Use resampling methods (oversampling, undersampling and both) to balance the training data.

* Evaluate using Precision, Recall, F1-Score, AUC, and Precision-Recall curves.

**2. Random Forest, XGBoost, LightGBM:**

* **Leave the dataset imbalanced.**

* Leverage built-in class-weight handling.

* Evaluate with Precision, Recall, F1-Score, AUC, and Precision-Recall curves.

**3. Logistic Regression:**

* **Use resampling techniques(oversampling, undersampling and both) to balance the training data.**

* Evaluate with the same metrics as above.

**4. Evaluation:**

* Test all models on the original, unaltered test set.

* Compare metrics to assess the impact of balancing and model performance.


## Naive Bayes

### Option 1: Adjust Class Priors to Handle Imbalance

Train Naive Bayes with Custom Priors

```{r}
table(train_y)
```


```{r}
# Adjust class priors (0.5 for both classes to equalize their weight)
custom_priors <- c(0.9193, 0.0807)

# Train Naive Bayes model with custom priors
set.seed(123)
nb_model <- naiveBayes(x = train_x, y = as.factor(train_y), prior = custom_priors)

# Predict probabilities and class labels
nb_probabilities <- predict(nb_model, test_x, type = "raw")[, 2]  # Probabilities for class 1
nb_predictions <- predict(nb_model, test_x)

# Evaluate using a confusion matrix
conf_matrix <- confusionMatrix(nb_predictions, as.factor(test_y), positive = "1")
print(conf_matrix)

```

```{r}
# Extract Precision, Recall, and calculate F1-Score
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score_nb <- 2 * ((precision * recall) / (precision + recall))
print(paste("F1-Score:", f1_score_nb))

# Calculate AUC and plot ROC Curve
roc_obj <- roc(as.factor(test_y), nb_probabilities, levels = c("0", "1"), positive = "1")
auc_value <- auc(roc_obj)
print(paste("AUC:", auc_value))
plot(roc_obj, main = "ROC Curve for Naive Bayes", col = "red")

# Calculate and plot Precision-Recall Curve
pr_curve <- pr.curve(scores.class0 = nb_probabilities, weights.class0 = as.numeric(test_y == "1"), curve = TRUE)
plot(pr_curve, main = "Precision-Recall Curve for Naive Bayes", col = "blue")

```

* F1-Score: 0.2366, 0.6706

* Precision-Recall Curve: Shows moderate balance but still struggles with minority class detection.

* Observations: The model is biased towards the majority class but performs better on overall metrics like F1-Score compared to the downsampled approach.


### Option 2: Use downsampling Methods to Balance Training Data

Balance dataset using downsampling

```{r}
# Perform downsampling
set.seed(123)  # Ensure reproducibility
train_data_balanced <- downSample(x = train_data %>% select(-target), 
                                  y = train_data$target, 
                                  yname = "target")

# Separate predictors and target again after downsampling
train_x_balanced <- train_data_balanced %>% select(-target)
train_y_balanced <- train_data_balanced$target

# Print summary of the balanced data
table(train_y_balanced)  # Check class distribution
```
```{r}
# Train Naive Bayes model on downsampled data
nb_model_downsampled <- naiveBayes(x = train_x_balanced, y = as.factor(train_y_balanced))

# Predict probabilities and class labels
nb_probabilities_downsampled <- predict(nb_model_downsampled, test_x, type = "raw")[, 2]
nb_predictions_downsampled <- predict(nb_model_downsampled, test_x)

# Evaluate using confusion matrix
conf_matrix_downsampled <- confusionMatrix(nb_predictions_downsampled, as.factor(test_y), positive = "1")
print(conf_matrix_downsampled)

```

```{r}
# Extract Precision, Recall, and calculate F1-Score for downsampled model
precision_down <- conf_matrix_downsampled$byClass["Pos Pred Value"]
recall_down <- conf_matrix_downsampled$byClass["Sensitivity"]
f1_score_down <- 2 * ((precision_down * recall_down) / (precision_down + recall_down))
print(paste("F1-Score (Downsampled):", f1_score_down))

# Calculate AUC and plot ROC Curve for downsampled model
roc_obj_downsampled <- roc(as.factor(test_y), nb_probabilities_downsampled, levels = c("0", "1"), positive = "1")
auc_value_downsampled <- auc(roc_obj_downsampled)
print(paste("AUC (Downsampled):", auc_value_downsampled))
plot(roc_obj_downsampled, main = "ROC Curve (Downsampled Naive Bayes)", col = "green")

# Calculate and plot Precision-Recall Curve for downsampled model
pr_curve_downsampled <- pr.curve(scores.class0 = nb_probabilities_downsampled, weights.class0 = as.numeric(test_y == "1"), curve = TRUE)
plot(pr_curve_downsampled, main = "Precision-Recall Curve (Downsampled Naive Bayes)", col = "purple")

```

* F1-Score:  0.2046, 0.6710

* Precision-Recall Curve: Indicates slightly improved recall but reduced precision.

* Observations: Downsampling improves sensitivity (recall) for the minority class but at the cost of precision, resulting in slightly worse F1-Score. Accuracy also drops due to the artificial balancing.

**Takeways:**

* Custom Priors: Better overall balance between precision and recall, maintaining higher accuracy and F1-Score.

* Downsampling: Helps capture more minority class instances (higher recall), but results in higher false positives, reducing precision and F1-Score.

### Option 3: Use oversampling method (Random OverSampling Examples) to balance Training Data

```{r}
colnames(train_x)

```

```{r}
# Combine predictors and target into one dataset
train_data <- cbind(train_x, target = train_y)

# Apply ROSE directly to the dataset
set.seed(123)  # For reproducibility
train_data_oversampled <- ROSE(target ~ ., data = train_data, seed = 123)$data

# Separate predictors and target again after oversampling
train_x_oversampled <- train_data_oversampled %>% select(-target)
train_y_oversampled <- train_data_oversampled$target

# Check class distribution to ensure oversampling worked
table(train_y_oversampled)
```



```{r}
# Train Naive Bayes model on oversampled data
nb_model_oversampled <- naiveBayes(x = train_x_oversampled, y = as.factor(train_y_oversampled))

# Predict probabilities and class labels
nb_probabilities_oversampled <- predict(nb_model_oversampled, test_x, type = "raw")[, 2]
nb_predictions_oversampled <- predict(nb_model_oversampled, test_x)

# Evaluate using confusion matrix
conf_matrix_oversampled <- confusionMatrix(nb_predictions_oversampled, as.factor(test_y), positive = "1")
print(conf_matrix_oversampled)

```


```{r}
# Calculate Precision, Recall, and F1-Score
precision_oversampled <- conf_matrix_oversampled$byClass["Pos Pred Value"]
recall_oversampled <- conf_matrix_oversampled$byClass["Sensitivity"]
f1_score_oversampled <- 2 * ((precision_oversampled * recall_oversampled) / (precision_oversampled + recall_oversampled))
print(paste("F1-Score (Oversampled):", f1_score_oversampled))

# Calculate AUC and plot ROC Curve
roc_obj_oversampled <- roc(as.factor(test_y), nb_probabilities_oversampled, levels = c("0", "1"), positive = "1")
auc_value_oversampled <- auc(roc_obj_oversampled)
print(paste("AUC (Oversampled):", auc_value_oversampled))
plot(roc_obj_oversampled, main = "ROC Curve (Oversampled Naive Bayes)", col = "blue")

# Calculate and plot Precision-Recall Curve
pr_curve_oversampled <- pr.curve(scores.class0 = nb_probabilities_oversampled, weights.class0 = as.numeric(test_y == "1"), curve = TRUE)
plot(pr_curve_oversampled, main = "Precision-Recall Curve (Oversampled Naive Bayes)", col = "green")

```
### Option 4: Use oversampling and downsampling Methods to Balance Training Data

```{r}
# Combine predictors and target into one dataset
train_data <- cbind(train_x, target = train_y)

# Apply both SMOTE (oversampling) and downsampling using ROSE
set.seed(123)  # For reproducibility
train_data_combined <- ROSE(target ~ ., data = train_data, p = 0.5, seed = 123)$data

# Separate predictors and target again after combining oversampling and downsampling
train_x_combined <- train_data_combined %>% select(-target)
train_y_combined <- train_data_combined$target

# Check class distribution to ensure combination worked
table(train_y_combined)

```
```{r}
# Train Naive Bayes model on combined dataset
nb_model_combined <- naiveBayes(x = train_x_combined, y = as.factor(train_y_combined))

# Predict probabilities and class labels
nb_probabilities_combined <- predict(nb_model_combined, test_x, type = "raw")[, 2]
nb_predictions_combined <- predict(nb_model_combined, test_x)

# Evaluate using confusion matrix
conf_matrix_combined <- confusionMatrix(nb_predictions_combined, as.factor(test_y), positive = "1")
print(conf_matrix_combined)

# Extract Precision, Recall, and calculate F1-Score
precision_combined <- conf_matrix_combined$byClass["Pos Pred Value"]
recall_combined <- conf_matrix_combined$byClass["Sensitivity"]
f1_score_combined <- 2 * ((precision_combined * recall_combined) / (precision_combined + recall_combined))
print(paste("F1-Score (Combined):", f1_score_combined))

# Calculate AUC and plot ROC Curve
roc_obj_combined <- roc(as.factor(test_y), nb_probabilities_combined, levels = c("0", "1"), positive = "1")
auc_value_combined <- auc(roc_obj_combined)
print(paste("AUC (Combined):", auc_value_combined))
plot(roc_obj_combined, main = "ROC Curve (Combined Naive Bayes)", col = "orange")

# Calculate and plot Precision-Recall Curve
pr_curve_combined <- pr.curve(scores.class0 = nb_probabilities_combined, weights.class0 = as.numeric(test_y == "1"), curve = TRUE)
plot(pr_curve_combined, main = "Precision-Recall Curve (Combined Naive Bayes)", col = "purple")

```

### Comparison table

```{r}
# Create a data frame for comparison
comparison_table <- data.frame(
  Model = c("NB Custom Priors", "NB Downsampling", "NB Oversampling", "NB Combined Sampling"),
  F1_Score = c(f1_score_nb, f1_score_down, f1_score_oversampled, f1_score_combined),
  AUC = c(auc_value, auc_value_downsampled, auc_value_oversampled, auc_value_combined),
  Sensitivity = c(recall, recall_down, recall_oversampled, recall_combined),
  Precision = c(precision, precision_down, precision_oversampled, precision_combined),
  Accuracy = c(conf_matrix$overall["Accuracy"], 
               conf_matrix_downsampled$overall["Accuracy"], 
               conf_matrix_oversampled$overall["Accuracy"],
               conf_matrix_combined$overall["Accuracy"])
)

# Print the comparison table
print(comparison_table)

```

* Custom Priors is the preferred method for maintaining a balance between performance and accuracy, especially for business cases where overall accuracy is critical.

* Use Downsampling or Oversampling if identifying minority class instances (sensitivity) is more important than overall accuracy.

* Combined Sampling offers no added value over oversampling; therefore, it may not be the optimal choice.

## Random Forest

```{r}
# Train a Random Forest with class weights
set.seed(123)
rf_model <- randomForest(
  x = train_x,
  y = train_y,
  classwt = c(1, sum(train_y == 0) / sum(train_y == 1)),  # Class weights
  trControl = trainControl(method = "cv", number = 3),
  ntree = 50, # Reduce the number of trees for memory constraints
  importance = TRUE
)

# Predictions
rf_predictions <- predict(rf_model, test_x)
rf_probabilities <- predict(rf_model, test_x, type = "prob")

# Evaluate performance
rf_confusion <- confusionMatrix(rf_predictions, test_y, positive = "1")  # Save output to a variable'
print(rf_confusion)

# Calculate metrics
roc_obj <- roc(as.factor(test_y), rf_probabilities[, 2], levels = c("0", "1"), positive = "1")
auc_value <- auc(roc_obj)

```



```{r}
pr_curve <- pr.curve(scores.class0 = rf_probabilities[, 2], weights.class0 = as.numeric(test_y == "1"), curve = TRUE)

# Extract precision, recall, F1-Score
rf_precision <- rf_confusion$byClass["Pos Pred Value"]
rf_recall <- rf_confusion$byClass["Sensitivity"]
rf_f1_score <- 2 * ((rf_precision * rf_recall) / (rf_precision + rf_recall))

# Print results
print(paste("AUC:", auc_value))
print(paste("F1-Score:", rf_f1_score))
plot(roc_obj, main = "ROC Curve for Random Forest", col = "blue")
plot(pr_curve, main = "Precision-Recall Curve for Random Forest", col = "green")
```

## XGBoost 

```{r}
# Prepare data for XGBoost
train_matrix <- xgb.DMatrix(data = as.matrix(train_x), label = as.numeric(train_y) - 1)
test_matrix <- xgb.DMatrix(data = as.matrix(test_x), label = as.numeric(test_y) - 1)

# Set parameters with class weights
params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  scale_pos_weight = sum(train_y == 0) / sum(train_y == 1)  # Class weights
)

# Train the model
set.seed(123)
xgb_model <- xgb.train(
  params = params,
  data = train_matrix,
  nrounds = 100,
  watchlist = list(val = test_matrix, train = train_matrix),
  early_stopping_rounds = 10
)

# Predictions
xgb_predictions <- predict(xgb_model, test_matrix)
xgb_class <- ifelse(xgb_predictions > 0.5, 1, 0)

# Evaluate performance
confusionMatrix(factor(xgb_class), test_y, positive = "1")

```
```{r}
# Calculate AUC for XGBoost
xgb_roc_obj <- roc(as.factor(test_y), xgb_predictions, levels = c("0", "1"), positive = "1")
xgb_auc_value <- auc(xgb_roc_obj)

# Calculate precision, recall, and F1-Score for XGBoost
xgb_confusion <- confusionMatrix(factor(xgb_class), test_y, positive = "1")
xgb_precision <- xgb_confusion$byClass["Pos Pred Value"]
xgb_recall <- xgb_confusion$byClass["Sensitivity"]
xgb_f1_score <- 2 * ((xgb_precision * xgb_recall) / (xgb_precision + xgb_recall))

# Print results
print(paste("AUC for XGBoost:", xgb_auc_value))
print(paste("F1-Score for XGBoost:", xgb_f1_score))
plot(xgb_roc_obj, main = "ROC Curve for XGBoost", col = "blue")

```

## LightGBM


```{r}
# Prepare data for LightGBM
lgb_train <- lgb.Dataset(data = as.matrix(train_x), label = as.numeric(train_y) - 1)
lgb_test <- lgb.Dataset(data = as.matrix(test_x), label = as.numeric(test_y) - 1)

# Set parameters for LightGBM
lgb_params <- list(
  objective = "binary",
  metric = "auc",
  scale_pos_weight = sum(train_y == 0) / sum(train_y == 1),  # Class weights
  learning_rate = 0.1,
  num_leaves = 31,
  feature_fraction = 0.8,
  bagging_fraction = 0.8,
  bagging_freq = 5
)

# Train LightGBM model
set.seed(123)
lgb_model <- lgb.train(
  params = lgb_params,
  data = lgb_train,
  nrounds = 100,
  valids = list(test = lgb_test),
  early_stopping_rounds = 10
)

# Predictions
lgb_predictions <- predict(lgb_model, as.matrix(test_x))
lgb_class <- ifelse(lgb_predictions > 0.5, 1, 0)

# Evaluate performance
lgb_confusion <- confusionMatrix(factor(lgb_class), test_y, positive = "1")

# Extract metrics
accuracy <- lgb_confusion$overall["Accuracy"]
kappa <- lgb_confusion$overall["Kappa"]
sensitivity <- lgb_confusion$byClass["Sensitivity"]
specificity <- lgb_confusion$byClass["Specificity"]
pos_pred_value <- lgb_confusion$byClass["Pos Pred Value"]
neg_pred_value <- lgb_confusion$byClass["Neg Pred Value"]
prevalence <- lgb_confusion$byClass["Prevalence"]
detection_rate <- lgb_confusion$byClass["Detection Rate"]
detection_prevalence <- lgb_confusion$byClass["Detection Prevalence"]
balanced_accuracy <- lgb_confusion$byClass["Balanced Accuracy"]

# Print the confusion matrix and statistics
print(lgb_confusion)

```
```{r}
# Calculate AUC
lgb_roc_obj <- roc(as.factor(test_y), lgb_predictions, levels = c("0", "1"), positive = "1")
lgb_auc <- auc(lgb_roc_obj)

# Print AUC
print(paste("AUC for LightGBM:", lgb_auc))

# Calculate F1-Score
precision <- lgb_confusion$byClass["Pos Pred Value"]  # Positive Predictive Value
recall <- lgb_confusion$byClass["Sensitivity"]       # Sensitivity (Recall)
f1_score <- 2 * ((precision * recall) / (precision + recall))

# Print F1-Score
print(paste("F1-Score for LightGBM:", f1_score))

# Plot ROC Curve
plot(lgb_roc_obj, main = "ROC Curve for LightGBM", col = "blue")

```
### Comparison table

```{r}
# Create a data frame to store the metrics for each model
comparison_table_2 <- data.frame(
  Model = c("Random Forest", "XGBoost", "LightGBM"),
  Accuracy = c(rf_confusion$overall["Accuracy"],
               xgb_confusion$overall["Accuracy"],
               lgb_confusion$overall["Accuracy"]),
  Sensitivity = c(rf_confusion$byClass["Sensitivity"],
                  xgb_confusion$byClass["Sensitivity"],
                  lgb_confusion$byClass["Sensitivity"]),
  Specificity = c(rf_confusion$byClass["Specificity"],
                  xgb_confusion$byClass["Specificity"],
                  lgb_confusion$byClass["Specificity"]),
  Precision = c(rf_confusion$byClass["Pos Pred Value"],
                xgb_confusion$byClass["Pos Pred Value"],
                lgb_confusion$byClass["Pos Pred Value"]),
  F1_Score = c(rf_f1_score, xgb_f1_score, f1_score),
  AUC = c(auc_value, xgb_auc_value, lgb_auc)
)

# Display the table in the desired format
print(comparison_table_2)

```


* **Random Forest:**

High accuracy (0.91) but extremely low sensitivity (0.0018), indicating it performs poorly in detecting the minority class.

* **XGBoost:**

Balanced sensitivity (0.598) and decent AUC (0.72). However, the F1-Score (0.25) suggests it struggles with balancing precision and recall.

* **LightGBM:**

**Best AUC (0.73) among tree-based models, showing strong discrimination power. It also has balanced sensitivity (0.66) and F1-Score (0.25), making it a strong candidate for scenarios needing better minority class detection.**

## Logistic regression

### Option 1: Upsampling the minority class (SMOTE)

```{r}
# Create a recipe
data_recipe <- recipe(target ~ ., data = train_data) %>%
  step_smote(target)  # Apply SMOTE

# Prep the recipe
smote_recipe <- prep(data_recipe, training = train_data)

# Bake the recipe to get the balanced data
balanced_train_data_smote <- bake(smote_recipe, new_data = NULL)

# Check class balance
table(balanced_train_data_smote$target)


```
```{r}
# Train logistic regression on SMOTE balanced data
smote_model <- glm(target ~ ., data = balanced_train_data_smote, family = "binomial")

# Make predictions on the test set
smote_probabilities <- predict(smote_model, newdata = test_x, type = "response")
smote_predictions <- ifelse(smote_probabilities > 0.5, 1, 0)

# Confusion matrix
smote_confusion <- confusionMatrix(factor(smote_predictions), factor(test_y), positive = "1")

# Calculate AUC
smote_roc <- roc(test_y, smote_probabilities)
smote_auc <- auc(smote_roc)

# Calculate F1 Score
smote_precision <- smote_confusion$byClass["Pos Pred Value"]
smote_recall <- smote_confusion$byClass["Sensitivity"]
smote_f1_score <- 2 * ((smote_precision * smote_recall) / (smote_precision + smote_recall))

# Print metrics
print(smote_confusion)
```
```{r}
print(paste("AUC for SMOTE:", smote_auc))
print(paste("F1 Score for SMOTE:", smote_f1_score))

# Plot ROC Curve
plot(smote_roc, main = "ROC Curve for Logistic Regression (SMOTE)", col = "blue")

```
### Option 2: Use ROSE for hybrid sampling

```{r}
# Apply ROSE for both downsampling and upsampling
balanced_train_data_rose <- ROSE(target ~ ., data = train_data, seed = 123)$data

# Check class balance
table(balanced_train_data_rose$target)

```

```{r}
# Train logistic regression on ROSE balanced data
rose_model <- glm(target ~ ., data = balanced_train_data_rose, family = "binomial")

# Make predictions on the test set
rose_probabilities <- predict(rose_model, newdata = test_x, type = "response")
rose_predictions <- ifelse(rose_probabilities > 0.5, 1, 0)

# Confusion matrix
rose_confusion <- confusionMatrix(factor(rose_predictions), factor(test_y), positive = "1")
# Calculate AUC
rose_roc <- roc(test_y, rose_probabilities)
rose_auc <- auc(rose_roc)

# Calculate F1 Score
rose_precision <- rose_confusion$byClass["Pos Pred Value"]
rose_recall <- rose_confusion$byClass["Sensitivity"]
rose_f1_score <- 2 * ((rose_precision * rose_recall) / (rose_precision + rose_recall))

# Print metrics
print(rose_confusion)


```


```{r}
print(paste("AUC for ROSE:", rose_auc))
print(paste("F1 Score for ROSE:", rose_f1_score))

# Plot ROC Curve
plot(rose_roc, main = "ROC Curve for Logistic Regression (ROSE)", col = "green")
```

### Comparison table

```{r}
# Create a data frame to store the metrics for each model
comparison_table_3 <- data.frame(
  Model = c("Logistic Regression (SMOTE)", "Logistic Regression (ROSE)"),
  F1_Score = c(smote_f1_score, rose_f1_score),
  AUC = c(smote_auc, rose_auc),
  Sensitivity = c(smote_confusion$byClass["Sensitivity"], rose_confusion$byClass["Sensitivity"]),
  Precision = c(smote_confusion$byClass["Pos Pred Value"], rose_confusion$byClass["Pos Pred Value"]),
  Accuracy = c(smote_confusion$overall["Accuracy"], rose_confusion$overall["Accuracy"])
)

# Display the table 
print(comparison_table_3)

```

* **Logistic Regression with SMOTE:**

Decent AUC (0.727) and a fair balance of sensitivity (0.66), indicating good performance in detecting the minority class. However, the precision is low, leading to an F1-Score of 0.248,suggesting challenges in balancing precision and recall.


* **Logistic Regression with ROSE:**

Similar AUC (0.726) and sensitivity (0.66) as SMOTE but slightly lower F1-Score (0.245) suggesting a slightly weaker balance between precision and recall.


## Logistic regression (SMOTE) implementing Lasso, Ridge and Elastic Net

Logistic Regression with SMOTE seems to have slightly better performance compared to ROSE, especially in terms of F1-Score, AUC, and Precision. I will be using Logistic Regression with SMOTE dataset for implementing Lasso, Ridge, and Elastic Net.
 
```{r}
# Prepare data
x_train <- model.matrix(target ~ ., balanced_train_data_smote)[, -1]  # Convert predictors to matrix
y_train <- balanced_train_data_smote$target                          # Target variable
x_test <- model.matrix(~ ., test_x)[, -1]                            # Test predictors

# Lasso Regression (alpha = 1)
set.seed(123)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")  # Cross-validation for Lasso
lasso_lambda <- lasso_model$lambda.min                                    # Optimal lambda
lasso_predictions <- predict(lasso_model, newx = x_test, s = lasso_lambda, type = "response")
lasso_class <- ifelse(lasso_predictions > 0.5, 1, 0)

# Ridge Regression (alpha = 0)
set.seed(123)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, family = "binomial")  # Cross-validation for Ridge
ridge_lambda <- ridge_model$lambda.min                                    # Optimal lambda
ridge_predictions <- predict(ridge_model, newx = x_test, s = ridge_lambda, type = "response")
ridge_class <- ifelse(ridge_predictions > 0.5, 1, 0)

# Elastic Net (alpha = 0.5)
set.seed(123)
elastic_net_model <- cv.glmnet(x_train, y_train, alpha = 0.5, family = "binomial")  # Cross-validation for Elastic Net
elastic_net_lambda <- elastic_net_model$lambda.min                                # Optimal lambda
elastic_net_predictions <- predict(elastic_net_model, newx = x_test, s = elastic_net_lambda, type = "response")
elastic_net_class <- ifelse(elastic_net_predictions > 0.5, 1, 0)

# Evaluate Performance
# Lasso metrics
lasso_confusion <- confusionMatrix(factor(lasso_class), factor(test_y), positive = "1")
lasso_auc <- auc(roc(test_y, as.vector(lasso_predictions)))
lasso_f1 <- 2 * ((lasso_confusion$byClass["Pos Pred Value"] * lasso_confusion$byClass["Sensitivity"]) /
                  (lasso_confusion$byClass["Pos Pred Value"] + lasso_confusion$byClass["Sensitivity"]))

# Ridge metrics
ridge_confusion <- confusionMatrix(factor(ridge_class), factor(test_y), positive = "1")
ridge_auc <- auc(roc(test_y, as.vector(ridge_predictions)))
ridge_f1 <- 2 * ((ridge_confusion$byClass["Pos Pred Value"] * ridge_confusion$byClass["Sensitivity"]) /
                  (ridge_confusion$byClass["Pos Pred Value"] + ridge_confusion$byClass["Sensitivity"]))

# Elastic Net metrics
elastic_net_confusion <- confusionMatrix(factor(elastic_net_class), factor(test_y), positive = "1")
elastic_net_auc <- auc(roc(test_y, as.vector(elastic_net_predictions)))
elastic_net_f1 <- 2 * ((elastic_net_confusion$byClass["Pos Pred Value"] * elastic_net_confusion$byClass["Sensitivity"]) /
                        (elastic_net_confusion$byClass["Pos Pred Value"] + elastic_net_confusion$byClass["Sensitivity"]))

```

```{r}
# Lasso Regression confusion matrix
print(lasso_confusion)
```
```{r}
# Ridge Regression Confusion matrix
print(ridge_confusion)
```

```{r}
#Elastic Net confusion matrix
print(elastic_net_confusion)
```


```{r}
# Print results
print(paste("Lasso AUC:", lasso_auc, "F1 Score:", lasso_f1))
print(paste("Ridge AUC:", ridge_auc, "F1 Score:", ridge_f1))
print(paste("Elastic Net AUC:", elastic_net_auc, "F1 Score:", elastic_net_f1))
```
### Comparison table

```{r}
# Create a data frame to store the metrics for each model
comparison_table_4 <- data.frame(
  Model = c("Logistic Regression (Lasso)", 
            "Logistic Regression (Ridge)", 
            "Logistic Regression (Elastic Net)"),
  F1_Score = c(lasso_f1, 
               ridge_f1, 
               elastic_net_f1),
  AUC = c(lasso_auc, 
          ridge_auc, 
          elastic_net_auc),
  Sensitivity = c(lasso_confusion$byClass["Sensitivity"], 
                  ridge_confusion$byClass["Sensitivity"], 
                  elastic_net_confusion$byClass["Sensitivity"]),
  Precision = c(lasso_confusion$byClass["Pos Pred Value"], 
                ridge_confusion$byClass["Pos Pred Value"], 
                elastic_net_confusion$byClass["Pos Pred Value"]),
  Accuracy = c(lasso_confusion$overall["Accuracy"], 
               ridge_confusion$overall["Accuracy"], 
               elastic_net_confusion$overall["Accuracy"])
)

# Display the table
print(comparison_table_4)

```

* **Logistic Regression (Lasso):**

Competitive accuracy (0.678) and sensitivity (0.66), making it a strong contender for detecting the minority class. Higher AUC (0.727) compared to SMOTE and ROSE, indicating better class separation ability. F1-Score (0.248) is marginally higher than SMOTE and ROSE, and it also adds the benefit of feature selection, which helps with model simplicity and interpretability.


* **Logistic Regression (Ridge):**

Accuracy (0.677) and sensitivity (0.657) are slightly lower than Lasso but still competitive. AUC (0.727) matches Lasso, showcasing similar discriminatory ability. F1-Score (0.248) is comparable to Lasso, but Ridge focuses on feature weighting rather than selection, making it less interpretable.

* **Logistic Regression (Elastic Net):**

Accuracy (0.678) and sensitivity (0.658) are on par with Lasso and Ridge. AUC (0.727) is consistent with other regularized models, showing solid discriminatory ability. F1-Score (0.248) is similar to Ridge, but Elastic Net combines feature selection and weighting, offering a balance between Lasso and Ridge without significant performance gains.
 
## Majority classifier

```{r}
# Find the majority class in the training set
majority_class <- as.character(names(which.max(table(train_y))))

# Predict the majority class for all test instances
majority_predictions <- rep(majority_class, length(test_y))

# Evaluate performance using a confusion matrix
majority_confusion <- confusionMatrix(factor(majority_predictions, levels = levels(test_y)), 
                                      factor(test_y, levels = levels(test_y)), 
                                      positive = "1")

# Print the confusion matrix and statistics
print(majority_confusion)

# Calculate Precision, Recall, and F1-Score
majority_precision <- majority_confusion$byClass["Pos Pred Value"]  # Positive Predictive Value
majority_recall <- majority_confusion$byClass["Sensitivity"]       # Sensitivity (Recall)

# F1-Score handling NaN (if precision + recall = 0, F1 is undefined)
if (is.nan(majority_precision) || is.nan(majority_recall) || (majority_precision + majority_recall) == 0) {
  majority_f1_score <- 0  # Define F1-Score as 0 when undefined
} else {
  majority_f1_score <- 2 * ((majority_precision * majority_recall) / (majority_precision + majority_recall))
}

# Print F1-Score
print(paste("F1-Score for Majority Classifier:", majority_f1_score))

# Acknowledge AUC limitation
print("AUC for Majority Classifier is undefined as there are no positive predictions.")

```

# Step 9: Model selection

To determine which model to use for predicting default risk (where 1 indicates default and 0 indicates no default), we must prioritize metrics that effectively identify defaults (1), especially sensitivity (the ability to correctly identify defaults) and AUC (the ability to separate defaulters from non-defaulters).


* LightGBM is the best choice due to its high AUC (0.73), balanced Sensitivity (0.66), and strong F1-Score (0.25).


# Step 10: Feature importance analysis

```{r}
# Extract and plot feature importance
importance <- lgb.importance(lgb_model, percentage = TRUE)

# Display top features
print(importance)

```

```{r fig.height=10, fig.width=10}
# Plot feature importance
library(ggplot2)
ggplot(importance, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance in LightGBM", x = "Features", y = "Importance (Gain)")
```


* **ext_source_3 and ext_source_2:**

These external scores are the most influential predictors. They likely represent external credit ratings or scores, which correlate strongly with the likelihood of default.

* **amt_credit:**

The total credit amount plays a key role, indicating that higher credit amounts may influence the probability of default.

* **days_birth and days_employed:**

These features suggest age and employment tenure. Older borrowers or those with longer employment histories might have lower default risks.

* **name_education_type_Higher.education:**

Education level appears to be predictive, possibly indicating more stable financial behaviors among borrowers with higher education.

* **region_population_relative:**

The population density of a borrower’s location may influence default risk, reflecting economic conditions.

* **days_id_publish and days_registration:**

These features may indicate the stability of a borrower’s identity or account registration, which might be tied to risk.


```{r}
str(train_x)
str(test_x)
```
# Step 11: Feature engineering 

## Train data

```{r}
# Apply feature engineering to train_data
# Combine the most important features to capture non-linear relationships.
train_data$ext_source_ratio <- train_data$ext_source_3 / (train_data$ext_source_2 + 1)
train_data$credit_per_income <- train_data$amt_credit / (train_data$amt_income_total + 1)
train_data$age_employment_ratio <- abs(train_data$days_birth) / (abs(train_data$days_employed) + 1)

train_data$amt_credit_bin <- cut(train_data$amt_credit, breaks = c(0, 50000, 100000, 200000, Inf), 
                                  labels = c("Low", "Medium", "High", "Very High"))
train_data$age_group <- cut(abs(train_data$days_birth) / 365, 
                            breaks = c(0, 25, 35, 50, 65, Inf), 
                            labels = c("Young", "Mid-Young", "Mid", "Mid-Old", "Old"))

# For time-based features like days_employed, calculate rolling averages or lagged values.
train_data$rolling_avg_employed <- zoo::rollmean(abs(train_data$days_employed), 
                                                 k = 3, fill = NA, align = "right")

# Replace NA values with 0 (or an appropriate value like the column mean or median)
train_data$rolling_avg_employed[is.na(train_data$rolling_avg_employed)] <- 0

train_data[is.na(train_data)] <- 0
```


```{r}
# After applying feature engineering, split the dataset into predictors (train_x/test_x) and target (train_y/test_y)
train_x <- train_data %>% select(-target)
train_y <- train_data$target
```

## Test data

```{r}
# Similarly, apply the same feature engineering steps to test_data
test_data$ext_source_ratio <- test_data$ext_source_3 / (test_data$ext_source_2 + 1)
test_data$credit_per_income <- test_data$amt_credit / (test_data$amt_income_total + 1)
test_data$age_employment_ratio <- abs(test_data$days_birth) / (abs(test_data$days_employed) + 1)

test_data$amt_credit_bin <- cut(test_data$amt_credit, breaks = c(0, 50000, 100000, 200000, Inf), 
                                labels = c("Low", "Medium", "High", "Very High"))
test_data$age_group <- cut(abs(test_data$days_birth) / 365, 
                           breaks = c(0, 25, 35, 50, 65, Inf), 
                           labels = c("Young", "Mid-Young", "Mid", "Mid-Old", "Old"))

test_data$rolling_avg_employed <- zoo::rollmean(abs(test_data$days_employed), 
                                                k = 3, fill = NA, align = "right")
# Replace NA values with 0 (or an appropriate value like the column mean or median)
test_data$rolling_avg_employed[is.na(test_data$rolling_avg_employed)] <- 0

test_data[is.na(test_data)] <- 0

# Extract predictors and target for the test set
test_x <- test_data %>% select(-target)
test_y <- test_data$target
```


```{r}
# One-hot encode categorical features in train_x and test_x
dummy_encoder <- dummyVars("~ .", data = train_x)

# Apply encoding to both train_x and test_x
train_x <- predict(dummy_encoder, train_x) %>% as.data.frame()
test_x <- predict(dummy_encoder, test_x) %>% as.data.frame()
# Verify column alignment
identical(colnames(train_x), colnames(test_x))  # Ensure both have the same columns

```

## Align predictors with application_test_encoded

```{r}
# Apply feature engineering to application_test_encoded
application_test_encoded$ext_source_ratio <- application_test_encoded$ext_source_3 / (application_test_encoded$ext_source_2 + 1)

application_test_encoded$credit_per_income <- application_test_encoded$amt_credit / (application_test_encoded$amt_income_total + 1)

application_test_encoded$age_employment_ratio <- abs(application_test_encoded$days_birth) / (abs(application_test_encoded$days_employed) + 1)

application_test_encoded$amt_credit_bin <- cut(application_test_encoded$amt_credit, 
                                       breaks = c(0, 50000, 100000, 200000, Inf), 
                                       labels = c("Low", "Medium", "High", "Very High"))

application_test_encoded$age_group <- cut(abs(application_test_encoded$days_birth) / 365, 
                                  breaks = c(0, 25, 35, 50, 65, Inf), 
                                  labels = c("Young", "Mid-Young", "Mid", "Mid-Old", "Old"))

application_test_encoded$amt_credit_bin <- factor(application_test_encoded$amt_credit_bin, 
                                          levels = c("Low", "Medium", "High", "Very High"))

application_test_encoded$age_group <- factor(application_test_encoded$age_group, 
                                     levels = c("Young", "Mid-Young", "Mid", "Mid-Old", "Old"))

# For rolling averages, handle time-based features
application_test_encoded$rolling_avg_employed <- zoo::rollmean(abs(application_test_encoded$days_employed), k = 3, fill = NA, align = "right")

application_test_encoded$rolling_avg_employed[is.na(application_test_encoded$rolling_avg_employed)] <- 0  # Fill NA with 0

```


```{r}
# One-hot encode categorical features in application_test_encoded

dummy_encoder <- dummyVars("~ .", data = application_test_encoded)

# Apply encoding to both train_x and test_x
application_test_encoded <- predict(dummy_encoder, application_test_encoded) %>% as.data.frame()

# Verify column alignment
identical(colnames(train_x), colnames(application_test_encoded))  # Ensure both have the same columns

```


```{r}
# Updated Model Training with Feature Engineering

lgb_train_2 <- lgb.Dataset(data = as.matrix(train_x), label = as.numeric(train_y) - 1)
lgb_test_2 <- lgb.Dataset(data = as.matrix(test_x), label = as.numeric(test_y) - 1)

# Train LightGBM with the same parameters
lgb_model_2 <- lgb.train(
  params = lgb_params,
  data = lgb_train_2,
  nrounds = 100,
  valids = list(test = lgb_test_2),
  early_stopping_rounds = 10
)

# Predictions and evaluation
lgb_predictions_2 <- predict(lgb_model_2, as.matrix(test_x))
lgb_class_2 <- ifelse(lgb_predictions_2 > 0.5, 1, 0)
lgb_confusion_2 <- confusionMatrix(factor(lgb_class_2), test_y, positive = "1")
print(lgb_confusion_2)

```
```{r}
#install.packages(("MLmetrics"))
library(MLmetrics)
# Calculate F1 Score
f1_score_lgb <- F1_Score(y_pred = factor(lgb_class_2, levels = c(0, 1)), 
                     y_true = factor(test_y, levels = c(0, 1)), 
                     positive = "1")
print(paste("F1 Score:", f1_score_lgb))

# Calculate AUC
roc_curve <- roc(test_y, lgb_predictions_2)
auc_value <- auc(roc_curve)
print(paste("AUC:", auc_value))
```

# Step 12: Adjusting threshold for internal exploration (not for Kaggle)

```{r}
# Adjust threshold for prediction
threshold <- 0.5037  # Adjust based on your evaluation
lgb_class_adjusted <- ifelse(lgb_predictions_2 > threshold, 1, 0)

# Recalculate confusion matrix
adjusted_confusion <- confusionMatrix(factor(lgb_class_adjusted), test_y, positive = "1")
print(adjusted_confusion)

```
```{r}
f1_score_adjusted <- F1_Score(y_pred = factor(lgb_class_adjusted, levels = c(0, 1)), 
                              y_true = factor(test_y, levels = c(0, 1)), 
                              positive = "1")
print(paste("F1 Score (Adjusted Threshold):", round(f1_score_adjusted, 4)))

# Calculate AUC
library(pROC)
roc_curve_adjusted <- roc(test_y, lgb_predictions_2)
auc_value_adjusted <- auc(roc_curve_adjusted)
print(paste("AUC (Adjusted Threshold):", round(auc_value_adjusted, 4)))

# Optional: Plot ROC Curve
plot(roc_curve_adjusted, main = "ROC Curve", col = "blue", lwd = 2)
```

```{r}
# Find optimal threshold based on Youden's J statistic
optimal_coords <- coords(roc_curve_adjusted, "best", ret = "threshold", best.method = "youden")
print(paste("Optimal Threshold (Youden):", optimal_coords))

```
This provides a balance between precision (avoiding false positives) and recall (minimizing false negatives).

## Comparison table

```{r}
# Create a data frame to store and compare metrics
comparison_table_5 <- data.frame(
  Model = c("LightGBM (Initial)", 
            "LightGBM (After Feature Engineering (FE))", 
            "LightGBM (FE + Threshold Adjusted)"),
  
  Accuracy = c(lgb_confusion$overall["Accuracy"], 
               lgb_confusion_2$overall["Accuracy"], 
               adjusted_confusion$overall["Accuracy"]),
  
  Sensitivity = c(lgb_confusion$byClass["Sensitivity"], 
                  lgb_confusion_2$byClass["Sensitivity"], 
                  adjusted_confusion$byClass["Sensitivity"]),
  
  Specificity = c(lgb_confusion$byClass["Specificity"], 
                  lgb_confusion_2$byClass["Specificity"], 
                  adjusted_confusion$byClass["Specificity"]),
  
  Precision = c(lgb_confusion$byClass["Pos Pred Value"], 
                lgb_confusion_2$byClass["Pos Pred Value"], 
                adjusted_confusion$byClass["Pos Pred Value"]),
  
  F1_Score = c(f1_score, 
               f1_score_lgb, 
               f1_score_adjusted),
  
  AUC = c(lgb_auc, 
          auc_value, 
          auc_value_adjusted),
  
  Optimal_Threshold =  0.5037  # Optimal threshold for adjusted
)

# Print the comparison table
print(comparison_table_5)


```

The LightGBM model with feature engineering and a threshold of 0.5037 offers a better "fit", offering an AUC of 0.7347, F1-Score of 0.2521, accuracy of 68.61%, sensitivity of 65.52%, and specificity of 68.88%. It balances predictive power and fairness by combining a balance in precision and recall.


# Step 13: Preparing the application_test for Prediction

## Verifying Feature Alignment

```{r}
# Ensure SK_ID_CURR is included and of the correct type
application_test_encoded$sk_id_curr <- application_test$sk_id_curr
application_test_encoded$sk_id_curr <- as.integer(application_test_encoded$sk_id_curr)

```


```{r}
# Get the feature names for both datasets and compare
train_x_features <- colnames(train_x)
test_encoded_features <- colnames(application_test_encoded)

# Identify missing or extra features
# Find features present in train_x but not in application_test_encoded
missing_in_test <- setdiff(train_x_features, test_encoded_features)
# Find features present in application_test_encoded but not in train_x
extra_in_test <- setdiff(test_encoded_features, train_x_features)

# Print the results
if (length(missing_in_test) == 0 && length(extra_in_test) == 0) {
  cat("The features in train_x and application_test_encoded are identical.\n")
} else {
  cat("Features missing in application_test_encoded:\n", missing_in_test, "\n")
  cat("Extra features in application_test_encoded:\n", extra_in_test, "\n")
}

```
## Generating Predictions and Creating Submission File

```{r}
# Ensure SK_ID_CURR is not used in prediction
app_test_x <- application_test_encoded %>% select(-sk_id_curr)

# Predict probabilities using the trained model
final_predictions <- predict(lgb_model_2, newdata = as.matrix(app_test_x))

# Create the submission file
submission <- data.frame(
  SK_ID_CURR = application_test_encoded$sk_id_curr,
  TARGET = final_predictions
)

# Save submission to CSV
write.csv(submission, "submission_1.csv", row.names = FALSE)

```

## Submission Results Visualization (Kaggle Score 1)

```{r, echo=FALSE}
library(knitr)
include_graphics("submission_1.png")

```



# Step 14: Parameter Tuning

To find the optimal parameters, I ran a grid search using the following configuration:

```{r eval=FALSE}
# Set a parameter grid
param_grid <- expand.grid(
  num_leaves = c(15, 31, 63),
  max_depth = c(5, 8, 10),
  learning_rate = c(0.01, 0.05, 0.1),
  feature_fraction = c(0.6, 0.8, 1.0),
  bagging_fraction = c(0.7, 0.8, 0.9)
)

# Loop through the grid and evaluate each combination
results <- data.frame()
for (i in 1:nrow(param_grid)) {
  params <- param_grid[i, ]
  model <- lgb.train(
    params = list(
      objective = "binary",
      metric = "auc",
      num_leaves = params$num_leaves,
      max_depth = params$max_depth,
      learning_rate = params$learning_rate,
      feature_fraction = params$feature_fraction,
      bagging_fraction = params$bagging_fraction,
      bagging_freq = 5
    ),
    data = lgb_train_2,
    nrounds = 500,
    valids = list(test = lgb_test_2),
    early_stopping_rounds = 20
  )
  auc <- model$best_score
  results <- rbind(results, cbind(params, auc))
}
# Print the best combination
best_params <- results[which.max(results$auc), ]
print(best_params)

```

- `num_leaves`: 15, 31, 63
- `max_depth`: 5, 8, 10
- `learning_rate`: 0.01, 0.05, 0.1
- `feature_fraction`: 0.6, 0.8, 1.0
- `bagging_fraction`: 0.7, 0.8, 0.9

## Best Parameters Summary

This grid search took approximately **1 hour** to complete, and the best parameter combination was:

| `num_leaves` | `max_depth` | `learning_rate` | `feature_fraction` | `bagging_fraction` | `auc`     |
|--------------|-------------|------------------|---------------------|--------------------|-----------|
| 172          | 15          | 0.05            | 0.6                 | 0.9                | 0.7374732 |

This result was used to train the following model:

# Step 15: Final Model Training with Tuned Parameters

```{r}
lgb_params_2 <- list(
  objective = "binary",
  metric = "auc",
  scale_pos_weight = sum(train_y == 0) / sum(train_y == 1),  # Class weights
  learning_rate = 0.01,  # Lower learning rate
  num_leaves = 172,  # Keep it moderately high
  max_depth = 5,  # Limit tree depth
  feature_fraction = 0.6,  
  bagging_fraction = 0.9,  
  bagging_freq = 5,  # Retain same bagging frequency
  lambda_l1 = 0.1,  # Add L1 regularization
  lambda_l2 = 0.1   # Add L2 regularization
)

# Train LightGBM with tuned parameters
lgb_model_3 <- lgb.train(
  params = lgb_params_2,
  data = lgb_train_2,
  nrounds = 500,  # Increase the number of rounds
  valids = list(test = lgb_test_2),
  early_stopping_rounds = 20,  # Use higher patience
  verbose = -1
)

```

## Generating Predictions and Creating Submission File


```{r}
# Predict probabilities using the trained model
final_predictions_2 <- predict(lgb_model_3, newdata = as.matrix(app_test_x))

# Create the submission file
submission_2 <- data.frame(
  SK_ID_CURR = application_test_encoded$sk_id_curr,
  TARGET = final_predictions_2
)

# Save submission to CSV
write.csv(submission_2, "submission_2.csv", row.names = FALSE)

```

## Submission Results Visualization (Kaggle score 2)

```{r, echo=FALSE}
library(knitr)
include_graphics("submission_2.png")

```

# Step 16:  Cross-Validation Results 

The cross-validation (lgb_model_cv) is independent of the final model (lgb_model_3). Cross-validation evaluates how well tuned the parameters are to generalize to unseen data. The final model is trained on the entire dataset and used for predictions.

```{r}
# Create 5 stratified folds
folds <- createFolds(train_y, k = 5, list = TRUE, returnTrain = TRUE)

cv_results <- data.frame()  # To store AUC for each fold

for (i in seq_along(folds)) {
  # Get train/test split
  train_indices <- folds[[i]]
  test_indices <- setdiff(seq_len(nrow(train_x)), train_indices)
  
  train_x_fold <- train_x[train_indices, ]
  train_y_fold <- train_y[train_indices]
  
  test_x_fold <- train_x[test_indices, ]
  test_y_fold <- train_y[test_indices]
  
  # Prepare LightGBM datasets
  lgb_train_fold <- lgb.Dataset(data = as.matrix(train_x_fold), label = as.numeric(train_y_fold) - 1)
  lgb_test_fold <- lgb.Dataset(data = as.matrix(test_x_fold), label = as.numeric(test_y_fold) - 1)
  
  # Train model on this fold
  lgb_model_cv <- lgb.train(
    params = lgb_params_2,
    data = lgb_train_fold,
    nrounds = 500,
    valids = list(test = lgb_test_fold),
    early_stopping_rounds = 20,
    verbose = -1
  )
  
  # Evaluate performance
  fold_auc <- lgb_model_cv$best_score
  cv_results <- rbind(cv_results, data.frame(Fold = i, AUC = fold_auc))
}

# Print cross-validation results
print(cv_results)
avg_auc <- mean(cv_results$AUC)
cat("Average AUC across folds:", avg_auc, "\n")


```
Cross-validation confirms that the tuned parameters (lgb_params_2) are effective, achieving an average AUC of 0.7350004 across 5 folds.


```{r eval=FALSE, include=FALSE}
# Save the application_test_encoded dataset as a CSV file
write.csv(application_test_encoded, "application_test_encoded.csv", row.names = FALSE)

```

# Business Analysis based on the predictions

## Add Predictions and Expected Risk

```{r}
# Add predicted probabilities to the dataset
application_test_encoded$predicted_default_probability <- final_predictions_2

# Calculate expected financial risk
application_test_encoded$expected_risk <- application_test_encoded$amt_credit * application_test_encoded$predicted_default_probability

```

## Classify Clients by Risk

```{r}
# Define risk categories
application_test_encoded$risk_category <- cut(
  application_test_encoded$predicted_default_probability,
  breaks = c(0, 0.3, 0.6, 1),
  labels = c("Low Risk", "Medium Risk", "High Risk"),
  right = FALSE
)

```


## Summarize Insights for Each Risk Category

```{r}
# Summarize total loans, average loans, and default probability by risk category
risk_summary <- application_test_encoded %>%
  group_by(risk_category) %>%
  summarise(
    total_loans = sum(amt_credit, na.rm = TRUE),
    avg_loan = mean(amt_credit, na.rm = TRUE),
    avg_default_probability = mean(predicted_default_probability, na.rm = TRUE),
    avg_income = mean(amt_income_total, na.rm = TRUE),
    avg_days_employed = mean(days_employed, na.rm = TRUE)
  )
print(risk_summary)

```

## Define Personas


```{r}
# Extract and display the 10 best predictors based on importance
importance_predictors <- lgb.importance(lgb_model_3, percentage = TRUE)
top_10_predictors <- importance_predictors[1:20, ]  # Select top 10 predictors
print(top_10_predictors)

```

###  Combine age_group Columns
```{r}
# Recreate age_group column from one-hot encoding
application_test_encoded <- application_test_encoded %>%
  mutate(
    age_group = case_when(
      `age_group.Young` == 1 ~ "Young",
      `age_group.Mid-Young` == 1 ~ "Mid-Young",
      `age_group.Mid` == 1 ~ "Mid",
      `age_group.Mid-Old` == 1 ~ "Mid-Old",
      `age_group.Old` == 1 ~ "Old",
      TRUE ~ "Unknown"
    )
  )


```

### Combine amt_credit_bin Columns

```{r}
# Recreate amt_credit_bin column from one-hot encoding
application_test_encoded <- application_test_encoded %>%
  mutate(
    amt_credit_bin = case_when(
      `amt_credit_bin.Low` == 1 ~ "Low",
      `amt_credit_bin.Medium` == 1 ~ "Medium",
      `amt_credit_bin.High` == 1 ~ "High",
      `amt_credit_bin.Very High` == 1 ~ "Very High",
      TRUE ~ "Unknown"
    )
  )

```

## Summarize top features for each risk category

```{r}
# Summarize top features for each risk category
persona_features <- application_test_encoded %>%
  group_by(risk_category) %>%
  summarise(
    avg_ext_source_ratio = mean(ext_source_ratio, na.rm = TRUE),
    common_age_group = names(sort(table(age_group), decreasing = TRUE))[1],  # Most common age group
    avg_days_employed = mean(days_employed, na.rm = TRUE),
    avg_loan = mean(amt_credit, na.rm = TRUE),
    avg_income = mean(amt_income_total, na.rm = TRUE),
    avg_amt_credit = mean(amt_credit, na.rm = TRUE),
    avg_credit_per_income = mean(credit_per_income, na.rm = TRUE),
    avg_days_id_publish = mean(days_id_publish, na.rm = TRUE),
    avg_age_employment_ratio = mean(age_employment_ratio, na.rm = TRUE),
    pct_higher_education = mean(name_education_type_Higher.education, na.rm = TRUE)
  )

```

## Visualize Personas

```{r}
personas_table <- persona_features %>%
  mutate(
    key_characteristics = case_when(
      risk_category == "Low Risk" ~ "Stable employment, high external scores, low credit-to-income ratio",
      risk_category == "Medium Risk" ~ "Moderate external scores, medium loan amounts, some employment gaps",
      risk_category == "High Risk" ~ "Low external scores, high credit-to-income ratio, less stable employment"
    )
  )

```

```{r}
print(personas_table)
```

This analysis highlights clear differences between risk categories based on external source ratios, employment stability, education levels, and loan behavior. These insights can guide product design, marketing strategies, and risk mitigation measures to improve portfolio quality and business profitability.

NOTE: Employment stability values (e.g, 92,783 days) suggest possible outliers. I didnt address this because it was the test data, so it is important to leave it like it is.

## Enhanced Personas (with simulations)

```{r}
# Step 1: Simulate repayment_delay_days based on risk_category
set.seed(42)
application_test_encoded <- application_test_encoded %>%
  mutate(
    repayment_delay_days = case_when(
      risk_category == "Low Risk" ~ rnorm(n(), mean = 2, sd = 1),
      risk_category == "Medium Risk" ~ rnorm(n(), mean = 10, sd = 5),
      risk_category == "High Risk" ~ rnorm(n(), mean = 30, sd = 15)
    ),
    repayment_delay_days = pmax(0, repayment_delay_days)  # Ensure non-negative values
  )

# Step 2: Summarize enriched personas with profitability and repayment metrics
profitability_personas <- application_test_encoded %>%
  group_by(risk_category) %>%
  summarise(
    avg_loan = mean(amt_credit, na.rm = TRUE),
    avg_income = mean(amt_income_total, na.rm = TRUE),
    avg_credit_to_income = mean(credit_per_income, na.rm = TRUE),
    avg_days_employed = mean(days_employed, na.rm = TRUE),
    avg_repayment_delay = mean(repayment_delay_days, na.rm = TRUE),
    common_age_group = names(sort(table(age_group), decreasing = TRUE))[1]
  ) %>%
  mutate(
    # Loan profitability metrics
    interest_rate = 0.08,  # Annual interest rate
    default_probability = case_when(
      risk_category == "Low Risk" ~ 0.01,
      risk_category == "Medium Risk" ~ 0.05,
      risk_category == "High Risk" ~ 0.20
    ),
    expected_interest_revenue = avg_loan * interest_rate,
    expected_loss = avg_loan * default_probability,
    net_profit = expected_interest_revenue - expected_loss,
    risk_adjusted_roi = net_profit / avg_loan * 100
  )

# Step 3: Display the combined results
profitability_personas %>%
  select(
    risk_category, common_age_group, avg_loan, avg_income, avg_repayment_delay,
    expected_interest_revenue, expected_loss, net_profit, risk_adjusted_roi
  )

```

# Client Profiles and Business Recommendations

**Low Risk Clients:**

* **Profile**

* Age Group: Mid-Old (51–65).

* Loan Size: $578,964 | Income: $190,187 | External Score: 0.3948.

* Employment: Very stable (92,783 days).

* Education: 39.6% with higher education.

* Repayment Behavior: Minimal repayment delay (2 days on average).

* **Profitability Metrics:** (Simulations)

* Expected Interest Revenue: $46,317.

* Expected Loss: $5,790.

* Net Profit: $40,527.

* Risk-Adjusted ROI: 7%.

* **Business Insight:**

* Offer premium loans with low rates and flexible terms.

* Target high-income, educated clients for investment products.


**Medium Risk Clients:**

* **Profile**

* Age Group: Mid (36–50).

* Loan Size: $507,660 | Income: $178,543 | External Score: 0.3137.

* Employment: Stable (68,056 days).

* Education: 22.9% with higher education.

* Repayment Behavior: Moderate repayment delay (10 days on average).

* **Profitability Metrics:** (Simulations)

* Expected Interest Revenue: $40,613.

* Expected Loss: $25,383.

* Net Profit: $15,230.

* Risk-Adjusted ROI: 3%.

* **Business Insight:**

* Provide standard loans with moderate rates.

* Introduce flexible repayment plans and credit counseling.

**High Risk Clients**

* **Profile:**

* Age Group: Mid (36–50).

* Loan Size: $442,692 | Income: $160,342 | External Score: 0.2123.

* Employment: Unstable (27,811 days).

* Education: 10.7% with higher education.

* Repayment Behavior: Significant repayment delay (30 days on average).

* **Profitability Metrics:** (Simulations)

* Expected Interest Revenue: $35,415.

* Expected Loss: $88,538.

* Net Profit: -$53,123 (negative).

* Risk-Adjusted ROI: -12% (loss-making).

* **Business Insight:**

* Limit loans to collateral-based products.

* Focus on short-term plans and require stricter credit checks.

* Introduce financial literacy programs to improve repayment behavior.

## Key Metrics to Focus On

1. Education: Higher education correlates with lower risk:
Low Risk: 39.6% | Medium Risk: 22.9% | High Risk: 10.7%.

2.External Score: Threshold of 0.3 is ideal for safer loans.

3. Employment Stability: Key for risk assessment:
Low Risk: 92,783 days | Medium Risk: 68,056 days | High Risk: 27,811 days.

4. Credit-to-Income Ratio:
Low Risk: 3.31 | Medium Risk: 3.13 | High Risk: 3.04.

SIMULATIONS

5. Profitability Varies with Risk: Low-risk clients yield the highest net profit and ROI, making them ideal for long-term loan products.

6. Repayment Behavior as a Key Indicator: Average repayment delay increases significantly with risk.

## Action Plan

**1. Low Risk:** Expand premium loan offerings and target high earners with targeted marketing campaing.

**2. Medium Risk:** Offer standard loans with education on financial literacy.

**3. High Risk:** Focus on collateral-based lending and short-term plans.